{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from src import transforms as T\n",
    "\n",
    "from src.unet.unet import Unet\n",
    "from src.unet.unet_module import UnetModule\n",
    "from src.mri_module import MriModule\n",
    "from src.subsample import create_mask_for_mask_type, RandomMaskFunc\n",
    "from src.mri_data import CombinedSliceDataset, AnnotatedSliceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path, hparams_file=None):\n",
    "    print(f'Loading model from checkpoint: {checkpoint_path}')\n",
    "    model = UnetModule.load_from_checkpoint(checkpoint_path, hparams_file=hparams_file, map_location='mps')\n",
    "    model.eval()\n",
    "    print(f'Model loaded from checkpoint: {checkpoint_path}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    from tqdm import tqdm\n",
    "    metric = 0\n",
    "    metrics = dict()\n",
    "    roi_len = 1\n",
    "    metrics['val_loss'] = 0\n",
    "    metrics['image_l1_loss'] = 0\n",
    "    metrics['image_ssim_loss'] = 0\n",
    "    metrics['roi_l1_loss'] = 0\n",
    "    metrics['roi_ssim_loss'] = 0\n",
    "    \n",
    "    print('Evaluating model...')\n",
    "    for batch in tqdm(dataloader):\n",
    "        with torch.no_grad():\n",
    "            output = model.validation_step_comparison(batch, batch_idx=0)\n",
    "            metrics['val_loss'] += output['val_loss'].item()\n",
    "            metrics['image_l1_loss'] += output['image_l1_loss'].item()\n",
    "            metrics['image_ssim_loss'] += output['image_ssim_loss'].item()\n",
    "            roi_l1_loss = output['roi_l1_loss'].item()\n",
    "            roi_ssim_loss = output['roi_ssim_loss'].item()\n",
    "            if not (roi_l1_loss == 0 and roi_ssim_loss == 0):\n",
    "                roi_len += 1              \n",
    "                metrics['roi_l1_loss'] += roi_l1_loss\n",
    "                metrics['roi_ssim_loss'] += roi_ssim_loss\n",
    "            metric += output['val_loss']\n",
    "    metric /= len(dataloader)\n",
    "    metrics['val_loss'] /= len(dataloader)\n",
    "    metrics['image_l1_loss'] /= len(dataloader)\n",
    "    metrics['image_ssim_loss'] /= len(dataloader)\n",
    "    metrics['roi_l1_loss'] /= roi_len\n",
    "    metrics['roi_ssim_loss'] /= roi_len\n",
    "    metrics['roi_len'] = roi_len\n",
    "    return metric, metrics\n",
    "\n",
    "\n",
    "configs = {\n",
    "    'data_dir': 'data/singlecoil_val',\n",
    "    'checkpoints': [\n",
    "        'logs/unet/unet_roi/checkpoints/epoch=9-step=347420.ckpt',\n",
    "        'logs/unet/unet_l1/checkpoints/epoch=9-step=347420.ckpt',\n",
    "    ],\n",
    "    'batch_size': 1,\n",
    "    'num_workers': 4,\n",
    "    'challenge':\"singlecoil\",\n",
    "    'mask_type':\"random\",  # \"random\" or \"equispaced_fraction\"\n",
    "    'center_fractions':[0.08],  # number of center lines to use in the mask\n",
    "    'accelerations':[4],  # acceleration rates to use for the mask\n",
    "}\n",
    "\n",
    "\n",
    "mask = create_mask_for_mask_type(\n",
    "    configs['mask_type'], configs['center_fractions'], configs['accelerations']\n",
    ")\n",
    "\n",
    "val_transform = T.UnetDataTransform(configs['challenge'], mask_func=mask)\n",
    "\n",
    "# validation_dataset = CombinedSliceDataset(\n",
    "#     roots=[Path(configs['data_dir'])],\n",
    "#     challenges=['singlecoil'],\n",
    "#     transforms=[val_transform]\n",
    "# )\n",
    "\n",
    "validation_dataset = AnnotatedSliceDataset(\n",
    "    root=Path(configs['data_dir']),\n",
    "    transform=val_transform,\n",
    "    challenge=configs['challenge'],\n",
    "    use_dataset_cache=False,\n",
    "    raw_sample_filter=None,\n",
    "    subsplit='knee',\n",
    "    multiple_annotation_policy='all',\n",
    "    \n",
    ")\n",
    "\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "# Use the line below to get a smaller validation set\n",
    "#samples = 150\n",
    "#validation_dataset = random_split(validation_dataset, [samples, len(validation_dataset) - samples], generator=gen)[0]\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=configs['batch_size'],\n",
    "    num_workers=configs['num_workers'],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint: logs/unet/unet_roi/checkpoints/epoch=9-step=347420.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lsantos/Projects/fastMRI/.venv/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoint: logs/unet/unet_roi/checkpoints/epoch=9-step=347420.ckpt\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7135/7135 [28:38<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: logs/unet/unet_roi/checkpoints/epoch=9-step=347420.ckpt, Validation Metric: 0.0030482891015708447, other metrics: {'val_loss': 0.00304829346690883, 'image_l1_loss': 0.2920762070437036, 'image_ssim_loss': 0.6643564193668111, 'roi_l1_loss': 0.0, 'roi_ssim_loss': 0.0, 'roi_len': 1}\n",
      "Loading model from checkpoint: logs/unet/unet_l1/checkpoints/epoch=9-step=347420.ckpt\n",
      "Model loaded from checkpoint: logs/unet/unet_l1/checkpoints/epoch=9-step=347420.ckpt\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 175/7135 [00:55<27:33,  4.21it/s] "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for checkpoint_path in configs['checkpoints']:\n",
    "    model = load_model_from_checkpoint(checkpoint_path)\n",
    "    val_metric, metrics = evaluate_model(model, validation_loader)\n",
    "    results[checkpoint_path] = val_metric\n",
    "    print(f\"Model: {checkpoint_path}, Validation Metric: {val_metric}, other metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0036901235580444336"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of all models:\n",
      "Model: logs/unet/unet_roi/checkpoints/epoch=9-step=347420.ckpt, Validation Metric: 0.003733583688735962\n",
      "Model: logs/unet/unet_l1/checkpoints/epoch=9-step=347420.ckpt, Validation Metric: 0.0036901235580444336\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSummary of all models:\")\n",
    "for checkpoint_path, val_metric in results.items():\n",
    "    print(f\"Model: {checkpoint_path}, Validation Metric: {val_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logs/unet/unet_roi/checkpoints/epoch=9-step=347420.ckpt': 0.003733583688735962,\n",
       " 'logs/unet/unet_l1/checkpoints/epoch=9-step=347420.ckpt': 0.0036901235580444336}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
